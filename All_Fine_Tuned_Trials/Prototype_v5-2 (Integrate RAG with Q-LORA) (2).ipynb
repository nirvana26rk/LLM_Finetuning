{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DwEV3Iig4n-o","outputId":"fb227e44-d765-43c7-9247-58b90f088a0c","executionInfo":{"status":"ok","timestamp":1728709628562,"user_tz":-480,"elapsed":6883,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: flash-attn in /usr/local/lib/python3.10/dist-packages (2.6.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.4.1+cu121)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n"]}],"source":["import torch\n","torch.cuda.empty_cache()\n","\n","!pip install -qU langchain tiktoken langchain_community langchain_chroma langchain-huggingface huggingface-hub sentence_transformers chromadb langchainhub transformers peft\n","!pip install flash-attn --no-build-isolation"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2886,"status":"ok","timestamp":1728709631445,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"},"user_tz":-480},"id":"QGx2rC4Z-bTz","outputId":"2eb58a2e-f515-4b4b-9e94-92c1c8c928e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from langchain_huggingface import HuggingFacePipeline\n","from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n","from langchain_chroma import Chroma\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","from peft import PeftModel, PeftConfig\n","from IPython.display import display, Markdown\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"mE5xhSl7bvqC","executionInfo":{"status":"ok","timestamp":1728709631445,"user_tz":-480,"elapsed":4,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[],"source":["# Function to calculate the total number of tokens in the vector database\n","def count_total_tokens_in_vectorstore(vectorstore, tokenizer):\n","    # Retrieve all documents from the vector store\n","    all_docs = vectorstore.get()['documents']\n","\n","    total_tokens = 0\n","\n","    # Iterate over each document and calculate the number of tokens\n","    for doc in all_docs:\n","        tokens_in_doc = len(tokenizer.encode(doc))  # Tokenize the document content (which is a string)\n","        total_tokens += tokens_in_doc\n","\n","\n","    return total_tokens"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["aa3132eca9f64aedadea2b41b744757c","31583c1fd99c4aaaae2208c34b1c05af","4e66820d7bac42f99ea390cc475ce983","7a097647f76b411180fa7fb071ad8b67","5dadc26c959e4777961aedec60b8dd26","662edf849b7349d685bdb90f6d3b15f0","a64d481be77c4c119bd52876fb749bbb","71708f3a00114e5c93511b5af17d4a13","d32e3fc0c7a14e11845521a060925b7d","6bd6a75d17d74e2f86a715cbf7017b03","4c68296944234891bfb150cf0a3cb454"]},"executionInfo":{"elapsed":8451,"status":"error","timestamp":1728709639892,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"},"user_tz":-480},"id":"sZq2XNAK_--a","outputId":"fbbe9061-a152-4070-9dda-59020a0ae046"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","bf16 tensors are supported.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa3132eca9f64aedadea2b41b744757c"}},"metadata":{}},{"output_type":"error","ename":"OSError","evalue":"Incorrect path_or_model_id: 'PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Phi3ForCausalLM(\n      (model): Phi3Model(\n        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n        (embed_dropout): Dropout(p=0.0, inplace=False)\n        (layers): ModuleList(\n          (0-9): 10 x Phi3DecoderLayer(\n            (self_attn): Phi3SdpaAttention(\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (qkv_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=9216, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=9216, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n            )\n            (mlp): Phi3MLP(\n              (gate_up_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=16384, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=16384, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=8192, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (activation_fn): SiLU()\n            )\n            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n          )\n          (10-31): 22 x Phi3DecoderLayer(\n            (self_attn): Phi3SdpaAttention(\n              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n              (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n              (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n            )\n            (mlp): Phi3MLP(\n              (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n              (activation_fn): SiLU()\n            )\n            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n          )\n        )\n        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n      )\n      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n    )\n  )\n)'. Please provide either the path to a local folder or the repo_id of a model on the Hub.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mREPO_ID_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34m\"Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Phi3ForCausalLM(\n      (model): Phi3Model(\n        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n        (embed_dropout): Dropout(p=0.0, inplace=False)\n        (layers): ModuleList(\n          (0-9): 10 x Phi3DecoderLayer(\n            (self_attn): Phi3SdpaAttention(\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (qkv_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=9216, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=9216, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n            )\n            (mlp): Phi3MLP(\n              (gate_up_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=16384, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=16384, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=8192, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (activation_fn): SiLU()\n            )\n            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n          )\n          (10-31): 22 x Phi3DecoderLayer(\n            (self_attn): Phi3SdpaAttention(\n              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n              (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n              (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n            )\n            (mlp): Phi3MLP(\n              (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n              (activation_fn): SiLU()\n            )\n            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n          )\n        )\n        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n      )\n      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n    )\n  )\n)'.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-77aa038cf959>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_bf16_support\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mlora_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"KunalRaghuvanshi/phi3_mini_qlora_chemical_eng\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{err}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHFValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;34mf\"Incorrect path_or_model_id: '{path_or_repo_id}'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         ) from e\n","\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: 'PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Phi3ForCausalLM(\n      (model): Phi3Model(\n        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n        (embed_dropout): Dropout(p=0.0, inplace=False)\n        (layers): ModuleList(\n          (0-9): 10 x Phi3DecoderLayer(\n            (self_attn): Phi3SdpaAttention(\n              (o_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (qkv_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=9216, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=9216, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n            )\n            (mlp): Phi3MLP(\n              (gate_up_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=16384, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=16384, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=8192, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=8192, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (activation_fn): SiLU()\n            )\n            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n          )\n          (10-31): 22 x Phi3DecoderLayer(\n            (self_attn): Phi3SdpaAttention(\n              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n              (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n              (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n            )\n            (mlp): Phi3MLP(\n              (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n              (activation_fn): SiLU()\n            )\n            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n          )\n        )\n        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n      )\n      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n    )\n  )\n)'. Please provide either the path to a local folder or the repo_id of a model on the Hub."]}],"source":["# Initialize embeddings\n","embedding_model_name = \"BAAI/bge-small-en-v1.5\"\n","embedding_model_kwargs = {\"device\": \"cuda\"}\n","embedding_encode_kwargs = {\"normalize_embeddings\": True}\n","hf = HuggingFaceBgeEmbeddings(\n","    model_name=embedding_model_name,\n","    model_kwargs=embedding_model_kwargs,\n","    encode_kwargs=embedding_encode_kwargs\n",")\n","\n","# Initialize vector store and retriever\n","vectorstore = Chroma(\n","    persist_directory=\"/content/drive/MyDrive/CITS5553_Capstone/vector1\",\n","    embedding_function=hf\n",")\n","retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n","\n","\n","# Check the available device\n","if torch.cuda.is_available():\n","    device = \"cuda\"\n","elif torch.backends.mps.is_available():\n","    device = \"mps\"\n","else:\n","    device = \"cpu\"\n","\n","print(f\"Using device: {device}\")\n","\n","# Check for bf16 support\n","is_bf16_support = False\n","try:\n","    tensor_bf16 = torch.tensor([1.0], dtype=torch.bfloat16, device=device)\n","    is_bf16_support = True\n","    print(\"bf16 tensors are supported.\")\n","except TypeError:\n","    print(\"bf16 tensors are not supported.\")\n","\n","# Load the base model and tokenizer\n","base_model = \"microsoft/Phi-3.5-mini-instruct\"\n","\n","\n","# Load the fine-tuned Phi3 mini model with LoRA\n","model = AutoModelForCausalLM.from_pretrained(base_model, return_dict=True, device_map=device, torch_dtype=torch.bfloat16 if is_bf16_support else torch.float16)\n","lora_model = PeftModel.from_pretrained(model, \"KunalRaghuvanshi/phi3_mini_qlora_chemical_eng\")\n","tokenizer = AutoTokenizer.from_pretrained(lora_model)\n","\n","\n","MAX_TOKENS = 1052988\n","MIN_TOKENS = 105298\n","\n","# Modify the pipeline initialization\n","def get_safe_max_tokens(vectorstore, tokenizer):\n","    total_tokens = count_total_tokens_in_vectorstore(vectorstore, tokenizer)\n","    print(\"Toatal Tokens\")\n","    print(total_tokens)\n","    max_new_tokens = min(max(total_tokens // 10, MIN_TOKENS), MAX_TOKENS)\n","    return max_new_tokens\n","\n","# Update the pipeline initialization\n","max_new_tokens = get_safe_max_tokens(vectorstore, tokenizer)\n","pipeline = pipeline(\n","    \"text-generation\",\n","    model=lora_model,\n","    tokenizer=tokenizer,\n","    max_new_tokens=max_new_tokens\n",")\n","llm = HuggingFacePipeline(pipeline=pipeline)"]},{"cell_type":"code","source":["all_docs = vectorstore.get()['documents']\n","print(f\"Number of documents in vector store: {len(all_docs)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SxXlhtnEFYQc","executionInfo":{"status":"ok","timestamp":1728709132081,"user_tz":-480,"elapsed":567,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}},"outputId":"1e38b63c-d9d9-480e-9144-c5415d14ab85"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of documents in vector store: 0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tnhbv16wAE_X"},"outputs":[],"source":["# Update the RAGChatModel class\n","class RAGChatModel:\n","    def __init__(self, retriever, llm, tokenizer):\n","        self.retriever = retriever\n","        self.llm = llm\n","        self.tokenizer = tokenizer\n","        total_tokens = count_total_tokens_in_vectorstore(vectorstore, tokenizer)\n","        self.max_token_limit = min(max(total_tokens // 10, MIN_TOKENS), MAX_TOKENS)\n","        self.current_token_count = 0\n","        self.template_standard = \"\"\"\n","        <|system|>\n","        Answer the question in detail. Provide all the relevant information based on the provided context.\n","        It is critical that you mention all page numbers where this information is found. Do not skip any page numbers.\n","\n","        Context: {context}\n","\n","        Providing all the page numbers is essential for the answer.\n","        <|end|>\n","\n","        <|user|>\n","        Question: {question}\n","        <|end|>\n","\n","        <|assistant|>\n","        \"\"\"\n","        self.template_exceeded = \"\"\"\n","        <|system|>\n","        Answer the question in detail; warn that information is not taken from the prescribed textbook and must provide the page numbers where they can find the correct information in the prescribed textbook.\n","\n","        Context: {context}\n","        Providing all the page numbers is essential for the answer.\n","        <|end|>\n","\n","        <|user|>\n","        Question: {question}\n","        <|end|>\n","\n","        <|assistant|>\n","        \"\"\"\n","\n","    def num_tokens_from_string(self, string: str) -> int:\n","        \"\"\"Returns the number of tokens in a text string using the tokenizer.\"\"\"\n","        return len(self.tokenizer.encode(string))\n","\n","    def format_docs(self, docs, full_content=True):\n","        \"\"\"Format the documents to be used as context in the prompt.\"\"\"\n","        if full_content:\n","            return \"\\n\\n\".join(f\"Information in Page number: {(doc.metadata['page']+1)}\\n{doc.page_content}\" for doc in docs)\n","        else:\n","            return \"Information available in prescribed textbook \" + \", \".join(f\"Page number: {doc.metadata['page']}\" for doc in docs)\n","\n","    def get_prompt(self, docs, question):\n","        \"\"\"Generate the prompt based on token count and context formatting.\"\"\"\n","        # Format the context with full content\n","        context = self.format_docs(docs, full_content=True)\n","        total_tokens_in_context = self.num_tokens_from_string(context)\n","\n","        # Add tokens to the running total\n","        self.current_token_count += total_tokens_in_context\n","\n","        # Decide whether to use full content or only page numbers\n","        if self.current_token_count > self.max_token_limit:\n","            print(\"Token limit exceeded. Information from prescribed textbook will not be used.\")\n","            # Reformat context to include only page numbers\n","            context = self.format_docs(docs, full_content=False)\n","            template = self.template_exceeded\n","        else:\n","            template = self.template_standard\n","\n","        # Create the prompt\n","        prompt = template.format(context=context, question=question)\n","        return prompt\n","\n","    def extract_clean_answer(self, raw_output):\n","        \"\"\"Extract only the answer from the raw output.\"\"\"\n","        assistant_tag = \"<|assistant|>\"\n","        if assistant_tag in raw_output:\n","            clean_answer = raw_output.split(assistant_tag)[-1].strip()\n","            return clean_answer\n","        return raw_output.strip()\n","\n","    def ask_question(self, question):\n","        \"\"\"Main function to retrieve relevant docs and generate a response.\"\"\"\n","        # Add fixed request for page numbers to the user's question\n","        question_with_page_request = f\"{question}. Please provide the page numbers in your answer.\"\n","\n","        # Retrieve relevant documents\n","        docs = self.retriever.invoke(question_with_page_request)\n","\n","        # Generate prompt based on token count\n","        prompt = self.get_prompt(docs, question_with_page_request)\n","\n","        # Pass the prompt to the LLM\n","        result = self.llm.generate([prompt])\n","\n","        # Extract the generated text\n","        raw_answer = result.generations[0][0].text\n","\n","        # Get the clean answer\n","        clean_answer = self.extract_clean_answer(raw_answer)\n","\n","        # Display the answer\n","        display(Markdown(clean_answer))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MAbb4UbqAVm6"},"outputs":[],"source":["# Initialize the RAGChatModel\n","rag_chat_model = RAGChatModel(retriever, llm, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7606,"status":"ok","timestamp":1728652847048,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"},"user_tz":-480},"id":"WJOMy2MJyls1","outputId":"67a8be8b-db95-48af-d452-f0d42c3562a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gradio\n","  Downloading gradio-5.0.1-py3-none-any.whl.metadata (15 kB)\n","Collecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.0)\n","Collecting ffmpy (from gradio)\n","  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting gradio-client==1.4.0 (from gradio)\n","  Downloading gradio_client-1.4.0-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n","Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.2)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart>=0.0.9 (from gradio)\n","  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n","Collecting ruff>=0.2.2 (from gradio)\n","  Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n","Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.31.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.0->gradio) (2024.6.1)\n","Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.0->gradio)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n","Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.38.6)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Downloading gradio-5.0.1-py3-none-any.whl (42.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.4.0-py3-none-any.whl (319 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n","Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n","Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, ffmpy, aiofiles, gradio-client, gradio\n","  Attempting uninstall: websockets\n","    Found existing installation: websockets 13.1\n","    Uninstalling websockets-13.1:\n","      Successfully uninstalled websockets-13.1\n","Successfully installed aiofiles-23.2.1 ffmpy-0.4.0 gradio-5.0.1 gradio-client-1.4.0 pydub-0.25.1 python-multipart-0.0.12 ruff-0.6.9 semantic-version-2.10.0 tomlkit-0.12.0 websockets-12.0\n"]}],"source":["!pip install gradio"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":645},"executionInfo":{"elapsed":3933,"status":"ok","timestamp":1728652850973,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"},"user_tz":-480},"id":"KSMx6Ps3yT5k","outputId":"bbf5d297-3949-4659-a9e3-93df3c192833"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://e7da1df7499d8afb70.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://e7da1df7499d8afb70.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":9}],"source":["import gradio as gr\n","\n","# Function to shorten the question for the chat history display\n","def get_short_overview(question, answer, max_length=50):\n","    \"\"\"Generate a short summary of the question for the chat history.\"\"\"\n","    # For simplicity, return the first few words of the question\n","    short_question = (question[:max_length] + '...') if len(question) > max_length else question\n","    return short_question\n","\n","# Function for the RAG model interaction\n","def ask_question_gradio(history, question):\n","    \"\"\"Main function to retrieve relevant docs and generate a response.\"\"\"\n","    # Retrieve relevant documents\n","    docs = rag_chat_model.retriever.invoke(question)\n","\n","    # Generate prompt based on token count\n","    prompt = rag_chat_model.get_prompt(docs, question)\n","\n","    # Pass the prompt to the LLM\n","    result = rag_chat_model.llm.generate([prompt])\n","\n","    # Extract the generated text\n","    raw_answer = result.generations[0][0].text\n","\n","    # Get the clean answer\n","    clean_answer = rag_chat_model.extract_clean_answer(raw_answer)\n","\n","    # Add the question and answer to the conversation history\n","    history.append((question, clean_answer))\n","\n","    # Generate a short summary for the chat history section (only from the question)\n","    short_overview = get_short_overview(question, clean_answer)\n","\n","    # Append the short overview to the chat history display (only questions' summaries)\n","    chat_history = \"\\n\\n\".join([get_short_overview(q, a) for q, a in history])\n","\n","    # Return the updated history, overview history for the left panel, and clear the user input\n","    return history, chat_history, \"\"  # The empty string clears the input box\n","\n","# Create a Gradio Blocks interface for chat-like interaction\n","with gr.Blocks() as demo:\n","    gr.Markdown(\n","        \"\"\"\n","        <h1 style='text-align: center;'>Lora ChemNerd</h1>\n","        <p style='text-align: center;'>Ask any question and get a response from the RAG model.</p>\n","        \"\"\",\n","    )\n","\n","    with gr.Row():\n","        # Sidebar for chat history\n","        with gr.Column(scale=1, min_width=200):\n","            gr.Markdown(\"### Chat History Overview\")\n","            history_display = gr.Textbox(label=\"Chat History\", lines=20, interactive=False)  # Non-editable\n","\n","        # Main Chatbot Area\n","        with gr.Column(scale=2):\n","            # Chatbot layout with conversation history\n","            chatbot = gr.Chatbot(label=\"Q-LORA ChemNerd Chat\")\n","\n","            # Text input for user questions\n","            with gr.Row():\n","                user_input = gr.Textbox(\n","                    placeholder=\"Ask your question...\",\n","                    label=\"Type your message here:\"\n","                )\n","\n","                # Button to submit the question\n","                submit_button = gr.Button(\"Send\")\n","\n","            # Maintain the conversation history\n","            history_state = gr.State([])\n","\n","            # Link the components: send input, update chatbot, clear textbox, and update history display\n","            submit_button.click(\n","                ask_question_gradio,\n","                inputs=[history_state, user_input],\n","                outputs=[chatbot, history_display, user_input],\n","                scroll_to_output=True\n","            )\n","            user_input.submit(\n","                ask_question_gradio,\n","                inputs=[history_state, user_input],\n","                outputs=[chatbot, history_display, user_input],\n","                scroll_to_output=True\n","            )\n","\n","# Launch the Gradio interface\n","demo.launch(share=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":524},"id":"5_rtxszlAY2E","outputId":"05c3f9f3-b8c8-40b5-aa80-4b8c4a42892f","executionInfo":{"status":"ok","timestamp":1728652932962,"user_tz":-480,"elapsed":81995,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Welcome to the RAG Chat Model! Ask any question (type 'exit' to quit):\n","\n","\n","\n","Your question: To heat a dodecane fluid with steam, what type of heat exchanger should be applied and why?\n"]},{"output_type":"stream","name":"stderr","text":["Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"To heat a dodecane fluid with steam, a shell and tube heat exchanger should be applied. This type of heat exchanger is suitable for this application because it allows for efficient heat transfer between the steam and the dodecane fluid. The shell and tube design facilitates the flow of steam through the tubes while the dodecane fluid flows around the tubes within the shell, allowing for effective heat exchange.\n\nThe specific reasons for choosing a shell and tube heat exchanger for heating dodecane with steam include its high heat transfer efficiency, ability to handle high pressure and temperature differences, and its versatility in handling various fluids. Additionally, the design of the shell and tube heat exchanger allows for easy maintenance and cleaning, which is important for maintaining the efficiency of the heat exchange process.\n\nFor detailed information on the application of shell and tube heat exchangers in heating dodecane with steam, refer to the following pages:\n\n- Page 10: Introduction to Heat Exchangers\n- Page 12: Types of Heat Exchangers\n- Page 15: Shell and Tube Heat Exchangers\n- Page 18: Heat Transfer in Shell and Tube Heat Exchangers\n- Page 20: Design Considerations for Shell and Tube Heat Exchangers\n- Page 22: Application of Shell and Tube Heat Exchangers in Industrial Processes\n\nThese pages provide a comprehensive overview of the principles, design, and application of shell and tube heat exchangers, including their suitability for heating dodecane with steam."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","Your question: exit\n","Exiting the chat.\n"]}],"source":["# Start the interactive chat\n","print(\"Welcome to the RAG Chat Model! Ask any question (type 'exit' to quit):\")\n","while True:\n","  print(\"\\n\\n\")\n","  question = input(\"Your question: \")\n","  if question.lower() == 'exit':\n","    print(\"Exiting the chat.\")\n","    break\n","  rag_chat_model.ask_question(question)"]},{"cell_type":"markdown","metadata":{"id":"c-1jL_wcZbB7"},"source":["How do we determine the breakthru point for an absorption bed?\n","\n","what is entropy from the perspective of a molecule?\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[{"file_id":"1sdiEq3cWwb9kY5Ft0e2aEgxrOCZVXxYO","timestamp":1728647887609},{"file_id":"1kU74NQOgketqIUjiLRlhmsSdRsGKDOCF","timestamp":1728634569649}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"aa3132eca9f64aedadea2b41b744757c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_31583c1fd99c4aaaae2208c34b1c05af","IPY_MODEL_4e66820d7bac42f99ea390cc475ce983","IPY_MODEL_7a097647f76b411180fa7fb071ad8b67"],"layout":"IPY_MODEL_5dadc26c959e4777961aedec60b8dd26"}},"31583c1fd99c4aaaae2208c34b1c05af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_662edf849b7349d685bdb90f6d3b15f0","placeholder":"​","style":"IPY_MODEL_a64d481be77c4c119bd52876fb749bbb","value":"Loading checkpoint shards: 100%"}},"4e66820d7bac42f99ea390cc475ce983":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_71708f3a00114e5c93511b5af17d4a13","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d32e3fc0c7a14e11845521a060925b7d","value":2}},"7a097647f76b411180fa7fb071ad8b67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bd6a75d17d74e2f86a715cbf7017b03","placeholder":"​","style":"IPY_MODEL_4c68296944234891bfb150cf0a3cb454","value":" 2/2 [00:03&lt;00:00,  1.68s/it]"}},"5dadc26c959e4777961aedec60b8dd26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"662edf849b7349d685bdb90f6d3b15f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a64d481be77c4c119bd52876fb749bbb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71708f3a00114e5c93511b5af17d4a13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d32e3fc0c7a14e11845521a060925b7d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6bd6a75d17d74e2f86a715cbf7017b03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c68296944234891bfb150cf0a3cb454":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
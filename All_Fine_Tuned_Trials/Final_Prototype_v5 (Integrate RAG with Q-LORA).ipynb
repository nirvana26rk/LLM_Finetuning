{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DwEV3Iig4n-o","outputId":"0db00ec2-0829-487e-ea98-5609346bc523","executionInfo":{"status":"ok","timestamp":1728711336130,"user_tz":-480,"elapsed":11592,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.45.2)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n","Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.25.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.9)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: flash-attn in /usr/local/lib/python3.10/dist-packages (2.6.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.4.1+cu121)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n"]}],"source":["import torch\n","torch.cuda.empty_cache()\n","!pip install -U bitsandbytes\n","!pip install transformers datasets accelerate peft datasets\n","!pip install -qU langchain tiktoken langchain_community langchain_chroma langchain-huggingface huggingface-hub sentence_transformers chromadb langchainhub transformers peft\n","!pip install flash-attn --no-build-isolation"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3059,"status":"ok","timestamp":1728711363404,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"},"user_tz":-480},"id":"QGx2rC4Z-bTz","outputId":"1f36c54a-c9c3-4660-80aa-2687aa8cd3e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from langchain_huggingface import HuggingFacePipeline\n","from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n","from langchain_chroma import Chroma\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorForLanguageModeling,\n","    pipeline\n",")\n","from peft import PeftModel, PeftConfig\n","from IPython.display import display, Markdown\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"mE5xhSl7bvqC","executionInfo":{"status":"ok","timestamp":1728711364968,"user_tz":-480,"elapsed":343,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[],"source":["# Function to calculate the total number of tokens in the vector database\n","def count_total_tokens_in_vectorstore(vectorstore, tokenizer):\n","    # Retrieve all documents from the vector store\n","    all_docs = vectorstore.get()['documents']\n","\n","    total_tokens = 0\n","\n","    # Iterate over each document and calculate the number of tokens\n","    for doc in all_docs:\n","        tokens_in_doc = len(tokenizer.encode(doc))  # Tokenize the document content (which is a string)\n","        total_tokens += tokens_in_doc\n","\n","    return total_tokens"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123,"referenced_widgets":["025deae0ea404123a264da2b51120720","259886363319488fb6103b4349d2bc01","d5984714420e410a87a026c27d9ec2b7","689c938b1e374a29b35fbb96a7f2cfc7","8d3934c53aa64fa3bf79d2c0cc0948aa","e3ccd2f2dd60454d95950780a992d78f","52c60839ab99461f91a528e6cdfc47f2","89287a6dbffb4fe09282a190e647a809","37cc6bffa74a42cbb5299c90fc820730","53bc441db3b5441881e116394a4dc0f3","08b29152f774404486620f0d5f1e380b"]},"executionInfo":{"elapsed":9232,"status":"ok","timestamp":1728711376634,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"},"user_tz":-480},"id":"sZq2XNAK_--a","outputId":"aed4a2be-213c-4120-8348-36752ab2d747"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","bf16 tensors are supported.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"025deae0ea404123a264da2b51120720"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"]}],"source":["# Initialize embeddings\n","embedding_model_name = \"BAAI/bge-small-en-v1.5\"\n","embedding_model_kwargs = {\"device\": \"cuda\"}\n","embedding_encode_kwargs = {\"normalize_embeddings\": True}\n","hf = HuggingFaceBgeEmbeddings(\n","    model_name=embedding_model_name,\n","    model_kwargs=embedding_model_kwargs,\n","    encode_kwargs=embedding_encode_kwargs\n",")\n","\n","# Initialize vector store and retriever\n","vectorstore = Chroma(\n","    persist_directory=\"/content/drive/MyDrive/RAG/vector1\",\n","    embedding_function=hf\n",")\n","retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n","\n","# Check the available device\n","if torch.cuda.is_available():\n","    device = \"cuda\"\n","elif torch.backends.mps.is_available():\n","    device = \"mps\"\n","else:\n","    device = \"cpu\"\n","\n","print(f\"Using device: {device}\")\n","\n","# Check for bf16 support\n","is_bf16_support = False\n","try:\n","    tensor_bf16 = torch.tensor([1.0], dtype=torch.bfloat16, device=device)\n","    is_bf16_support = True\n","    print(\"bf16 tensors are supported.\")\n","except TypeError:\n","    print(\"bf16 tensors are not supported.\")\n","\n","# Quantization configuration\n","# Quantization configuration\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","# Load the base model and tokenizer\n","base_model = \"microsoft/Phi-3.5-mini-instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(base_model)\n","\n","# Load the fine-tuned Phi3 mini model with LoRA\n","model = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=bnb_config,return_dict=True, device_map=device)\n","\n","qlora_model = PeftModel.from_pretrained(model, \"KunalRaghuvanshi/phi3_mini_qlora_chemical_eng\")\n","\n","pipeline = pipeline(\"text-generation\", model=qlora_model, tokenizer=tokenizer, max_new_tokens=count_total_tokens_in_vectorstore(vectorstore, tokenizer)//10)\n","llm = HuggingFacePipeline(pipeline=pipeline)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"tnhbv16wAE_X","executionInfo":{"status":"ok","timestamp":1728711385598,"user_tz":-480,"elapsed":1436,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[],"source":["# Define the RAG Chat Model class\n","class RAGChatModel:\n","    def __init__(self, retriever, llm, tokenizer, max_token_limit=count_total_tokens_in_vectorstore(vectorstore, tokenizer)//10):\n","        self.retriever = retriever\n","        self.llm = llm\n","        self.tokenizer = tokenizer\n","        self.max_token_limit = max_token_limit\n","        self.current_token_count = 0\n","        self.template_standard = \"\"\"\n","        <|system|>\n","        Answer the question and mustgive all the page numbers for the answer where this information is found based in the information provided in the context.\n","        Providing all the page numbers is essential for the answer.\n","\n","        Context: {context}\n","\n","        Providing all the page numbers is essential  for the answer.\n","        <|end|>\n","\n","        <|user|>\n","        Question: {question}\n","        <|end|>\n","\n","        <|assistant|>\n","        \"\"\"\n","        self.template_exceeded = \"\"\"\n","        <|system|>\n","        Answer the question in detail; warn that information is not taken from the prescribed textbook and must provide the page numbers where they can find the correct information in the prescribed textbook.\n","\n","        Context: {context}\n","        Providing all the page numbers is essential for the answer.\n","        <|end|>\n","\n","        <|user|>\n","        Question: {question}\n","        <|end|>\n","\n","        <|assistant|>\n","        \"\"\"\n","\n","    def num_tokens_from_string(self, string: str) -> int:\n","        \"\"\"Returns the number of tokens in a text string using the tokenizer.\"\"\"\n","        return len(self.tokenizer.encode(string))\n","\n","    def format_docs(self, docs, full_content=True):\n","        \"\"\"Format the documents to be used as context in the prompt.\"\"\"\n","        if full_content:\n","            return \"\\n\\n\".join(f\"Information in Page number: {(doc.metadata['page']+1)}\\n{doc.page_content}\" for doc in docs)\n","        else:\n","            return \"Information available in prescribed textbook \" + \", \".join(f\"Page number: {doc.metadata['page']}\" for doc in docs)\n","\n","    def get_prompt(self, docs, question):\n","        \"\"\"Generate the prompt based on token count and context formatting.\"\"\"\n","        # Format the context with full content\n","        context = self.format_docs(docs, full_content=True)\n","        total_tokens_in_context = self.num_tokens_from_string(context)\n","\n","        # Add tokens to the running total\n","        self.current_token_count += total_tokens_in_context\n","\n","        # Decide whether to use full content or only page numbers\n","        if self.current_token_count > self.max_token_limit:\n","            print(\"Token limit exceeded. Information from prescribed textbook will not be used.\")\n","            # Reformat context to include only page numbers\n","            context = self.format_docs(docs, full_content=False)\n","            template = self.template_exceeded\n","        else:\n","            template = self.template_standard\n","\n","        # Create the prompt\n","        prompt = template.format(context=context, question=question)\n","        return prompt\n","\n","    def extract_clean_answer(self, raw_output):\n","        \"\"\"Extract only the answer from the raw output.\"\"\"\n","        assistant_tag = \"<|assistant|>\"\n","        if assistant_tag in raw_output:\n","            clean_answer = raw_output.split(assistant_tag)[-1].strip()\n","            return clean_answer\n","        return raw_output.strip()\n","\n","    def ask_question(self, question):\n","        \"\"\"Main function to retrieve relevant docs and generate a response.\"\"\"\n","        # Retrieve relevant documents\n","        docs = self.retriever.invoke(question)\n","\n","        # Generate prompt based on token count\n","        prompt = self.get_prompt(docs, question)\n","\n","        # Pass the prompt to the LLM\n","        result = self.llm.generate([prompt])\n","\n","        # Extract the generated text\n","        raw_answer = result.generations[0][0].text\n","\n","        # Get the clean answer\n","        clean_answer = self.extract_clean_answer(raw_answer)\n","\n","        # Display the answer\n","        display(Markdown(clean_answer))\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"MAbb4UbqAVm6","executionInfo":{"status":"ok","timestamp":1728711389832,"user_tz":-480,"elapsed":440,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[],"source":["# Initialize the RAGChatModel\n","rag_chat_model = RAGChatModel(retriever, llm, tokenizer)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8273,"status":"ok","timestamp":1728711399677,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"},"user_tz":-480},"id":"WJOMy2MJyls1","outputId":"2bdbb575-b4a0-4bc0-9087-1d50b98134fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gradio\n","  Downloading gradio-5.0.2-py3-none-any.whl.metadata (15 kB)\n","Collecting aiofiles<24.0,>=22.0 (from gradio)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.0)\n","Collecting ffmpy (from gradio)\n","  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting gradio-client==1.4.0 (from gradio)\n","  Downloading gradio_client-1.4.0-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n","Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.2)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n","Collecting pydub (from gradio)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart>=0.0.9 (from gradio)\n","  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n","Collecting ruff>=0.2.2 (from gradio)\n","  Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting semantic-version~=2.0 (from gradio)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting tomlkit==0.12.0 (from gradio)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n","Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.31.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.0->gradio) (2024.6.1)\n","Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.0->gradio)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n","Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.38.6)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Downloading gradio-5.0.2-py3-none-any.whl (42.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.4.0-py3-none-any.whl (319 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n","Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n","Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, ffmpy, aiofiles, gradio-client, gradio\n","  Attempting uninstall: websockets\n","    Found existing installation: websockets 13.1\n","    Uninstalling websockets-13.1:\n","      Successfully uninstalled websockets-13.1\n","Successfully installed aiofiles-23.2.1 ffmpy-0.4.0 gradio-5.0.2 gradio-client-1.4.0 pydub-0.25.1 python-multipart-0.0.12 ruff-0.6.9 semantic-version-2.10.0 tomlkit-0.12.0 websockets-12.0\n"]}],"source":["!pip install gradio"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":650},"executionInfo":{"elapsed":3858,"status":"ok","timestamp":1728711405235,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"},"user_tz":-480},"id":"KSMx6Ps3yT5k","outputId":"fb86437e-f3e2-437a-a178-0ef2192580f1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://47c2a79969243499f8.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://47c2a79969243499f8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":14}],"source":["import gradio as gr\n","\n","# Function to shorten the question for the chat history display\n","def get_short_overview(question, answer, max_length=50):\n","    \"\"\"Generate a short summary of the question for the chat history.\"\"\"\n","    # For simplicity, return the first few words of the question\n","    short_question = (question[:max_length] + '...') if len(question) > max_length else question\n","    return short_question\n","\n","# Function for the RAG model interaction\n","def ask_question_gradio(history, question):\n","    \"\"\"Main function to retrieve relevant docs and generate a response.\"\"\"\n","    # Retrieve relevant documents\n","    docs = rag_chat_model.retriever.invoke(question)\n","\n","    # Generate prompt based on token count\n","    prompt = rag_chat_model.get_prompt(docs, question)\n","\n","    # Pass the prompt to the LLM\n","    result = rag_chat_model.llm.generate([prompt])\n","\n","    # Extract the generated text\n","    raw_answer = result.generations[0][0].text\n","\n","    # Get the clean answer\n","    clean_answer = rag_chat_model.extract_clean_answer(raw_answer)\n","\n","    # Add the question and answer to the conversation history\n","    history.append((question, clean_answer))\n","\n","    # Generate a short summary for the chat history section (only from the question)\n","    short_overview = get_short_overview(question, clean_answer)\n","\n","    # Append the short overview to the chat history display (only questions' summaries)\n","    chat_history = \"\\n\\n\".join([get_short_overview(q, a) for q, a in history])\n","\n","    # Return the updated history, overview history for the left panel, and clear the user input\n","    return history, chat_history, \"\"  # The empty string clears the input box\n","\n","# Create a Gradio Blocks interface for chat-like interaction\n","with gr.Blocks() as demo:\n","    gr.Markdown(\n","        \"\"\"\n","        <h1 style='text-align: center;'>Q-LORA ChemNerd</h1>\n","        <p style='text-align: center;'>Ask any question and get a response from the RAG model.</p>\n","        \"\"\",\n","    )\n","\n","    with gr.Row():\n","        # Sidebar for chat history\n","        with gr.Column(scale=1, min_width=200):\n","            gr.Markdown(\"### Chat History Overview\")\n","            history_display = gr.Textbox(label=\"Chat History\", lines=20, interactive=False)  # Non-editable\n","\n","        # Main Chatbot Area\n","        with gr.Column(scale=2):\n","            # Chatbot layout with conversation history\n","            chatbot = gr.Chatbot(label=\"Q-LORA ChemNerd Chat\")\n","\n","            # Text input for user questions\n","            with gr.Row():\n","                user_input = gr.Textbox(\n","                    placeholder=\"Ask your question...\",\n","                    label=\"Type your message here:\"\n","                )\n","\n","                # Button to submit the question\n","                submit_button = gr.Button(\"Send\")\n","\n","            # Maintain the conversation history\n","            history_state = gr.State([])\n","\n","            # Link the components: send input, update chatbot, clear textbox, and update history display\n","            submit_button.click(\n","                ask_question_gradio,\n","                inputs=[history_state, user_input],\n","                outputs=[chatbot, history_display, user_input],\n","                scroll_to_output=True\n","            )\n","            user_input.submit(\n","                ask_question_gradio,\n","                inputs=[history_state, user_input],\n","                outputs=[chatbot, history_display, user_input],\n","                scroll_to_output=True\n","            )\n","\n","# Launch the Gradio interface\n","demo.launch(share=True)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":978},"id":"5_rtxszlAY2E","outputId":"d35ac89f-63c7-44d5-eae2-3bbd6994f845","executionInfo":{"status":"ok","timestamp":1728711602257,"user_tz":-480,"elapsed":193097,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Welcome to the RAG Chat Model! Ask any question (type 'exit' to quit):\n","\n","\n","\n","Your question: How many stages are required to distill toluene from decane at atmospheric conditions\n"]},{"output_type":"stream","name":"stderr","text":["Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"To determine the number of stages required to distill toluene from decane at atmospheric conditions, we need to consider the relative volatility of toluene to decane and the desired purity of the toluene product. The relative volatility (α) is a measure of how easily one component can be separated from another in a mixture by distillation. It is defined as the ratio of the vapor pressures of the two components.\n\nGiven that the relative volatility of toluene to decane is not provided in the context, we would typically look up this value in a chemical engineering textbook or database. However, for the sake of this explanation, let's assume a hypothetical relative volatility value, which we'll denote as α.\n\nThe Fenske equation is commonly used to estimate the minimum number of theoretical stages (N_min) required for binary distillation under total reflux conditions. The equation is given by:\n\nN_min = log((xD/xB) * (yB/yD)) / log(α)\n\nWhere:\n- N_min is the minimum number of theoretical stages,\n- xD is the mole fraction of the more volatile component in the distillate,\n- xB is the mole fraction of the more volatile component in the bottoms,\n- yD is the mole fraction of the more volatile component in the distillate vapor,\n- yB is the mole fraction of the more volatile component in the bottoms vapor,\n- α is the relative volatility of the more volatile component to the less volatile component.\n\nAssuming we want to achieve a distillate with 98% toluene (xD = 0.98) and a bottoms with 2% toluene (xB = 0.02), and assuming the vapor phase mole fractions (yD and yB) are equal to the liquid phase mole fractions due to the high purity of toluene in the distillate, we can simplify the equation to:\n\nN_min = log((0.98/0.02) * (0.02/0.98)) / log(α)\n\nWithout the specific value of α, we cannot calculate the exact number of stages. However, once α is known, you can plug in the values and solve for N_min.\n\nKeep in mind that this is a simplified approach and actual distillation processes may require more stages due to inefficiencies not accounted for in the Fenske equation. Additionally, the presence of other components in the mixture, even in small amounts, can affect the relative volatility and thus the number of stages required.\n\nFor a more accurate determination, one would typically use process simulation software like Aspen HYSYS, which can account for non-idealities and provide a more realistic estimate of the number of stages needed for the separation process.\n\nIn summary, without the specific relative volatility value, we cannot provide a numerical answer. However, the approach outlined above, using the Fenske equation, gives a theoretical framework for estimating the minimum number of stages required for distilling toluene from decane under atmospheric conditions."},"metadata":{}},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","Your question: How do we determine the breakthru point for an absorption bed?\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"To determine the breakthrough point for an absorption bed, we utilize the concept of the mass-transfer zone and the break-point time. The break-point time, denoted as $t_{b}$, is calculated using the ideal time and the fraction of bed capacity utilized at the break point. This is done by integrating the breakthrough curve up to the break point time. The total solute adsorbed up to the break point is obtained by integrating a complete breakthrough curve or from separate equilibrium tests. The capacity of the solid is then determined by this integration. The ratio of the total solute adsorbed to the capacity of the solid gives us the fraction of the bed capacity utilized at the break point. Subtracting this fraction from 1.0 gives us the unused fraction, which is converted to an equivalent length of unused bed (LUB). This length, assumed to be constant, is then used to predict the break point by applying the equation (25.17), where $c / c_{0}$ is set at a value like 0.05, and the length of the saturated bed is calculated as the product of transfer zone velocity and the time since the zone started to move.\n\n        Providing all the page numbers is essential for the answer."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","Your question: exit\n","Exiting the chat.\n"]}],"source":["# Start the interactive chat\n","print(\"Welcome to the RAG Chat Model! Ask any question (type 'exit' to quit):\")\n","while True:\n","  print(\"\\n\\n\")\n","  question = input(\"Your question: \")\n","  if question.lower() == 'exit':\n","    print(\"Exiting the chat.\")\n","    break\n","  rag_chat_model.ask_question(question)"]},{"cell_type":"markdown","metadata":{"id":"c-1jL_wcZbB7"},"source":["How do we determine the breakthru point for an absorption bed?\n","\n","what is entropy from the perspective of a molecule?\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[{"file_id":"1ctZe_beYWunSL3f3qnES85zs_X997Jvz","timestamp":1728709811945},{"file_id":"1kU74NQOgketqIUjiLRlhmsSdRsGKDOCF","timestamp":1728709092928}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"025deae0ea404123a264da2b51120720":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_259886363319488fb6103b4349d2bc01","IPY_MODEL_d5984714420e410a87a026c27d9ec2b7","IPY_MODEL_689c938b1e374a29b35fbb96a7f2cfc7"],"layout":"IPY_MODEL_8d3934c53aa64fa3bf79d2c0cc0948aa"}},"259886363319488fb6103b4349d2bc01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3ccd2f2dd60454d95950780a992d78f","placeholder":"​","style":"IPY_MODEL_52c60839ab99461f91a528e6cdfc47f2","value":"Loading checkpoint shards: 100%"}},"d5984714420e410a87a026c27d9ec2b7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_89287a6dbffb4fe09282a190e647a809","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_37cc6bffa74a42cbb5299c90fc820730","value":2}},"689c938b1e374a29b35fbb96a7f2cfc7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53bc441db3b5441881e116394a4dc0f3","placeholder":"​","style":"IPY_MODEL_08b29152f774404486620f0d5f1e380b","value":" 2/2 [00:05&lt;00:00,  2.48s/it]"}},"8d3934c53aa64fa3bf79d2c0cc0948aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3ccd2f2dd60454d95950780a992d78f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52c60839ab99461f91a528e6cdfc47f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89287a6dbffb4fe09282a190e647a809":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37cc6bffa74a42cbb5299c90fc820730":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53bc441db3b5441881e116394a4dc0f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08b29152f774404486620f0d5f1e380b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DwEV3Iig4n-o","outputId":"850d16e2-c147-4629-cd02-a039973e080b","executionInfo":{"status":"ok","timestamp":1728399690757,"user_tz":-480,"elapsed":6944,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: flash-attn in /usr/local/lib/python3.10/dist-packages (2.6.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.4.1+cu121)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n"]}],"source":["import torch\n","torch.cuda.empty_cache()\n","\n","!pip install -qU langchain tiktoken langchain_community langchain_chroma langchain-huggingface huggingface-hub sentence_transformers chromadb langchainhub transformers peft\n","!pip install flash-attn --no-build-isolation"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QGx2rC4Z-bTz","outputId":"7261cc0f-ce07-4be7-99ef-b38e1d2c36de","executionInfo":{"status":"ok","timestamp":1728399693489,"user_tz":-480,"elapsed":2736,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from langchain_huggingface import HuggingFacePipeline\n","from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n","from langchain_chroma import Chroma\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","from peft import PeftModel, PeftConfig\n","from IPython.display import display, Markdown\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"mE5xhSl7bvqC","executionInfo":{"status":"ok","timestamp":1728399693489,"user_tz":-480,"elapsed":3,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[],"source":["# Function to calculate the total number of tokens in the vector database\n","import pdb\n","def count_total_tokens_in_vectorstore(vectorstore, tokenizer):\n","    # Retrieve all documents from the vector store\n","    all_docs = vectorstore.get()['documents']\n","\n","    total_tokens = 0\n","\n","    # Iterate over each document and calculate the number of tokens\n","    for doc in all_docs:\n","        tokens_in_doc = len(tokenizer.encode(doc))  # Tokenize the document content (which is a string)\n","        total_tokens += tokens_in_doc\n","        pdb.set_trace()\n","    return total_tokens"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["53378b435f384a4ead9f94e12bae32e9","6d2f7b3891fb4e36a744694dd5d3fd93","829f653c0cd04199bac6401a2529b2c1","4f8ba9cc875d40baaeda01b637f64580","b9948b84873249be81a06835c7127534","430505fc2e89496688486e54abee3b99","3c15721834c549d695de3cd6954b2928","e6ff51c1c04245f8b9c7b2801e5aefad","f574e640f56046f9b3a8b8f5e55c67a4","e9aeb279838d418b8a07b88f670ad193","5d26a111ae7f45bdaf087e0af98540cc"]},"id":"sZq2XNAK_--a","outputId":"e8bb17a7-57c3-4b77-bb81-c5d0658d0b94","executionInfo":{"status":"ok","timestamp":1728399734383,"user_tz":-480,"elapsed":7968,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","bf16 tensors are supported.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53378b435f384a4ead9f94e12bae32e9"}},"metadata":{}}],"source":["# Initialize embeddings\n","embedding_model_name = \"BAAI/bge-small-en-v1.5\"\n","embedding_model_kwargs = {\"device\": \"cuda\"}\n","embedding_encode_kwargs = {\"normalize_embeddings\": True}\n","hf = HuggingFaceBgeEmbeddings(\n","    model_name=embedding_model_name,\n","    model_kwargs=embedding_model_kwargs,\n","    encode_kwargs=embedding_encode_kwargs\n",")\n","\n","# Initialize vector store and retriever\n","vectorstore = Chroma(\n","    persist_directory=\"/content/drive/MyDrive/CITS5553_Capstone/vector1\",\n","    embedding_function=hf\n",")\n","retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n","\n","# Check the available device\n","if torch.cuda.is_available():\n","    device = \"cuda\"\n","elif torch.backends.mps.is_available():\n","    device = \"mps\"\n","else:\n","    device = \"cpu\"\n","\n","print(f\"Using device: {device}\")\n","\n","# Check for bf16 support\n","is_bf16_support = False\n","try:\n","    tensor_bf16 = torch.tensor([1.0], dtype=torch.bfloat16, device=device)\n","    is_bf16_support = True\n","    print(\"bf16 tensors are supported.\")\n","except TypeError:\n","    print(\"bf16 tensors are not supported.\")\n","\n","# Load the base model and tokenizer\n","base_model = \"microsoft/Phi-3.5-mini-instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(base_model)\n","\n","\n","# Load the base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    return_dict=True,\n","    device_map=device,\n","    torch_dtype=torch.bfloat16 if is_bf16_support else torch.float16\n",")\n","\n","# Load and merge the LoRA weights\n","lora_model = PeftModel.from_pretrained(model, \"KunalRaghuvanshi/phi3_mini_qlora_chemical_eng\")\n","merged_model = lora_model.merge_and_unload()\n","\n","pipeline = pipeline(\n","    \"text-generation\",\n","    model=merged_model,\n","    tokenizer=tokenizer,\n","    max_new_tokens=max(400, count_total_tokens_in_vectorstore(vectorstore, tokenizer) // 10)\n",")\n","\n","llm = HuggingFacePipeline(pipeline=pipeline)\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"tnhbv16wAE_X","executionInfo":{"status":"ok","timestamp":1728399743024,"user_tz":-480,"elapsed":630,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[],"source":["class RAGChatModel:\n","    def __init__(self, retriever, llm, tokenizer, min_tokens=400, max_tokens= max(400, count_total_tokens_in_vectorstore(vectorstore, tokenizer) // 10)):\n","        self.retriever = retriever\n","        self.llm = llm\n","        self.tokenizer = tokenizer\n","\n","        # Calculate max_token_limit with bounds\n","        total_tokens = count_total_tokens_in_vectorstore(vectorstore, tokenizer)\n","        suggested_tokens = max(min_tokens, total_tokens // 10)\n","        self.max_token_limit = min(suggested_tokens, max_tokens)\n","\n","        self.current_token_count = 0\n","        self.template_standard = \"\"\"\n","        <|system|>\n","        Answer the question and mustgive all the page numbers for the answer where this information is found based in the information provided in the context.\n","        Providing all the page numbers is essential for the answer.\n","\n","        Context: {context}\n","\n","        Providing all the page numbers is essential  for the answer.\n","        <|end|>\n","\n","        <|user|>\n","        Question: {question}\n","        <|end|>\n","\n","        <|assistant|>\n","        \"\"\"\n","        self.template_exceeded = \"\"\"\n","        <|system|>\n","        Answer the question in detail; warn that information is not taken from the prescribed textbook and must provide the page numbers where they can find the correct information in the prescribed textbook.\n","\n","        Context: {context}\n","        Providing all the page numbers is essential for the answer.\n","        <|end|>\n","\n","        <|user|>\n","        Question: {question}\n","        <|end|>\n","\n","        <|assistant|>\n","        \"\"\"\n","\n","    def num_tokens_from_string(self, string: str) -> int:\n","        \"\"\"Returns the number of tokens in a text string using the tokenizer.\"\"\"\n","        return len(self.tokenizer.encode(string))\n","\n","    def format_docs(self, docs, full_content=True):\n","        \"\"\"Format the documents to be used as context in the prompt.\"\"\"\n","        if full_content:\n","            return \"\\n\\n\".join(f\"Information in Page number: {(doc.metadata['page']+1)}\\n{doc.page_content}\" for doc in docs)\n","        else:\n","            return \"Information available in prescribed textbook \" + \", \".join(f\"Page number: {doc.metadata['page']}\" for doc in docs)\n","\n","    def get_prompt(self, docs, question):\n","        \"\"\"Generate the prompt based on token count and context formatting.\"\"\"\n","        # Format the context with full content\n","        context = self.format_docs(docs, full_content=True)\n","        total_tokens_in_context = self.num_tokens_from_string(context)\n","\n","        # Add tokens to the running total\n","        self.current_token_count += total_tokens_in_context\n","\n","        # Decide whether to use full content or only page numbers\n","        if self.current_token_count > self.max_token_limit:\n","            print(\"Token limit exceeded. Information from prescribed textbook will not be used.\")\n","            # Reformat context to include only page numbers\n","            context = self.format_docs(docs, full_content=False)\n","            template = self.template_exceeded\n","        else:\n","            template = self.template_standard\n","\n","        # Create the prompt\n","        prompt = template.format(context=context, question=question)\n","        return prompt\n","\n","    def extract_clean_answer(self, raw_output):\n","        \"\"\"Extract only the answer from the raw output.\"\"\"\n","        assistant_tag = \"<|assistant|>\"\n","        if assistant_tag in raw_output:\n","            clean_answer = raw_output.split(assistant_tag)[-1].strip()\n","            return clean_answer\n","        return raw_output.strip()\n","\n","    def ask_question(self, question):\n","        \"\"\"Main function to retrieve relevant docs and generate a response.\"\"\"\n","        # Retrieve relevant documents\n","        docs = self.retriever.invoke(question)\n","\n","        # Generate prompt based on token count\n","        prompt = self.get_prompt(docs, question)\n","\n","        # Pass the prompt to the LLM\n","        result = self.llm.generate([prompt])\n","\n","        # Extract the generated text\n","        raw_answer = result.generations[0][0].text\n","\n","        # Get the clean answer\n","        clean_answer = self.extract_clean_answer(raw_answer)\n","\n","        # Display the answer\n","        display(Markdown(clean_answer))\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"MAbb4UbqAVm6","executionInfo":{"status":"ok","timestamp":1728399747039,"user_tz":-480,"elapsed":563,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[],"source":["\n","# Initialize the RAGChatModel with explicit bounds\n","rag_chat_model = RAGChatModel(\n","    retriever=retriever,\n","    llm=llm,\n","    tokenizer=tokenizer,\n","    min_tokens=400,    # Minimum number of tokens to generate\n","    max_tokens=1000    # Maximum number of tokens to generate\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"5_rtxszlAY2E","outputId":"22780f40-2ff9-4aa5-c123-3687ecf73aeb","executionInfo":{"status":"ok","timestamp":1728400288937,"user_tz":-480,"elapsed":532168,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Welcome to the RAG Chat Model! Ask any question (type 'exit' to quit):\n","\n","\n","\n","Your question: what is entropy from the perspective of a molecule?\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Entropy, from the perspective of a molecule, is a measure of the disorder or randomness in a system. It is a fundamental concept in thermodynamics and statistical mechanics, which helps to explain the spontaneous direction of processes and the distribution of energy within a system.\n\nIn the context of a molecule, entropy can be thought of as the number of ways the molecule's energy can be distributed among its various degrees of freedom (such as translational, rotational, and vibrational motions). A higher entropy state implies a greater number of possible configurations for the molecule's energy, leading to increased disorder.\n\nThe concept of entropy is quantified by the Boltzmann's entropy formula, S = k * ln(W), where S is the entropy, k is the Boltzmann constant, and W is the number of microstates corresponding to a particular macrostate of the system.\n\nEntropy plays a crucial role in determining the spontaneity of chemical reactions and phase transitions. For instance, in a chemical reaction, the change in entropy (∆S) along with the change in enthalpy (∆H) determines the Gibbs free energy change (∆G) of the reaction, which predicts whether the reaction will occur spontaneously.\n\nThe increase in entropy is a driving force for many natural processes, including the diffusion of molecules, the melting of solids, and the evaporation of liquids. These processes lead to a more disordered state, which is thermodynamically favored.\n\nIn summary, from the perspective of a molecule, entropy represents the degree of disorder or randomness in the distribution of energy among its various degrees of freedom. It is a key concept in understanding the behavior of molecules and the direction of chemical processes.\n\nPage numbers:"},"metadata":{}},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","Your question: what is an ideal gas law? Please provide the page numbers\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The ideal gas law is a fundamental equation in chemistry and physics that describes the behavior of an ideal gas. It is represented by the formula PV = nRT, where P stands for pressure, V for volume, n for the number of moles of gas, R for the ideal gas constant, and T for temperature in Kelvin.\n\nThe ideal gas law assumes that the gas particles are point particles with no volume and that there are no intermolecular forces between them. This law is an approximation that works well under many conditions, especially at high temperatures and low pressures.\n\nPage numbers for the ideal gas law can be found in the following sections of the text:\n\n- Introduction to Gases: Pages 1-2\n- The Kinetic Theory of Gases: Pages 3-5\n- The Ideal Gas Law: Pages 6-8\n\nPlease note that the page numbers provided are hypothetical and for illustrative purposes only. In a real context, you would refer to the actual textbook or document to find the precise page numbers."},"metadata":{}},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","Your question: How do we determine the breakthru point for an absorption bed?\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"To determine the breakthru point for an absorption bed, we refer to the provided context which indicates that the breakthru point is a critical parameter in the design and operation of absorption systems. The breakthru point is the stage at which the solute concentration in the solvent reaches a maximum and begins to decrease as the solvent continues to flow through the bed.\n\n\nAccording to the context, the breakthru point can be calculated using the following equation:\n\n\n\\[ \\text{Breakthru Point} = \\frac{\\text{Total Capacity} \\times \\text{Solvent Flow Rate}}{\\text{Solvent Flow Rate} + \\text{Solvent Flow Rate at Breakthru Point}} \\]\n\n\nThis equation is derived from the principles of mass transfer and the design of absorption columns. The total capacity represents the maximum amount of solute that the bed can absorb, while the solvent flow rate is the rate at which the solvent passes through the bed. The solvent flow rate at the breakthru point is the rate at which the solvent flow changes as it reaches the maximum solute absorption capacity.\n\n\nThe breakthru point is significant because it indicates the efficiency of the absorption process and helps in optimizing the design of the absorption bed. By understanding the breakthru point, engineers can adjust the solvent flow rate and other operational parameters to ensure that the absorption bed operates within its optimal range, thereby maximizing the efficiency of the solute removal process.\n\n\nPage numbers for the detailed explanation of the breakthru point calculation and its significance in absorption bed design are not provided in the context. However, the equation and its components are essential for determining the breakthru point in absorption processes.\n\n\nAnswer: The breakthru point for an absorption bed is determined using the"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n","\n","Your question: exit\n","Exiting the chat.\n"]}],"source":["# Start the interactive chat\n","print(\"Welcome to the RAG Chat Model! Ask any question (type 'exit' to quit):\")\n","while True:\n","  print(\"\\n\\n\")\n","  question = input(\"Your question: \")\n","  if question.lower() == 'exit':\n","    print(\"Exiting the chat.\")\n","    break\n","  rag_chat_model.ask_question(question)"]},{"cell_type":"markdown","metadata":{"id":"c-1jL_wcZbB7"},"source":["How do we determine the breakthru point for an absorption bed?\n","\n","what is entropy from the perspective of a molecule?\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[{"file_id":"1kU74NQOgketqIUjiLRlhmsSdRsGKDOCF","timestamp":1728343323656}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"53378b435f384a4ead9f94e12bae32e9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d2f7b3891fb4e36a744694dd5d3fd93","IPY_MODEL_829f653c0cd04199bac6401a2529b2c1","IPY_MODEL_4f8ba9cc875d40baaeda01b637f64580"],"layout":"IPY_MODEL_b9948b84873249be81a06835c7127534"}},"6d2f7b3891fb4e36a744694dd5d3fd93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_430505fc2e89496688486e54abee3b99","placeholder":"​","style":"IPY_MODEL_3c15721834c549d695de3cd6954b2928","value":"Loading checkpoint shards: 100%"}},"829f653c0cd04199bac6401a2529b2c1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6ff51c1c04245f8b9c7b2801e5aefad","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f574e640f56046f9b3a8b8f5e55c67a4","value":2}},"4f8ba9cc875d40baaeda01b637f64580":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9aeb279838d418b8a07b88f670ad193","placeholder":"​","style":"IPY_MODEL_5d26a111ae7f45bdaf087e0af98540cc","value":" 2/2 [00:03&lt;00:00,  1.46s/it]"}},"b9948b84873249be81a06835c7127534":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"430505fc2e89496688486e54abee3b99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c15721834c549d695de3cd6954b2928":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6ff51c1c04245f8b9c7b2801e5aefad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f574e640f56046f9b3a8b8f5e55c67a4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e9aeb279838d418b8a07b88f670ad193":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d26a111ae7f45bdaf087e0af98540cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13334,"status":"ok","timestamp":1729089765008,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"},"user_tz":-480},"id":"DwEV3Iig4n-o","outputId":"76f3be73-388d-46f1-ece2-fa5504c8415c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.45.2)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n","Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.25.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.14.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: flash-attn in /usr/local/lib/python3.10/dist-packages (2.6.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.4.1+cu121)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n"]}],"source":["import torch\n","torch.cuda.empty_cache()\n","!pip install -U bitsandbytes\n","!pip install transformers datasets accelerate peft datasets\n","!pip install -qU langchain tiktoken langchain_community langchain_chroma langchain-huggingface huggingface-hub sentence_transformers chromadb langchainhub transformers peft\n","!pip install flash-attn --no-build-isolation"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9359,"status":"ok","timestamp":1729089774351,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"},"user_tz":-480},"id":"QGx2rC4Z-bTz","outputId":"83ba87df-b779-4451-e532-c5a927b98cd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from langchain_huggingface import HuggingFacePipeline\n","from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n","from langchain_chroma import Chroma\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorForLanguageModeling,\n","    pipeline\n",")\n","from peft import PeftModel, PeftConfig\n","from IPython.display import display, Markdown\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"mE5xhSl7bvqC","executionInfo":{"status":"ok","timestamp":1729089774352,"user_tz":-480,"elapsed":8,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[],"source":["# Function to calculate the total number of tokens in the vector database\n","def count_total_tokens_in_vectorstore(vectorstore, tokenizer):\n","    # Retrieve all documents from the vector store\n","    all_docs = vectorstore.get()['documents']\n","\n","    total_tokens = 0\n","\n","    # Iterate over each document and calculate the number of tokens\n","    for doc in all_docs:\n","        tokens_in_doc = len(tokenizer.encode(doc))  # Tokenize the document content (which is a string)\n","        total_tokens += tokens_in_doc\n","\n","    return total_tokens"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225,"referenced_widgets":["47a72627d0b64c8588a8f540ecb8b06f","494cde9578904a14af109517dd1f06ec","fd69bda058c541d0b17800797b7301ab","78a541c71ad34026bbf4727ac7ea7999","006c1689411e4f4fa2d03656e5f86f48","3154a1a68f9243549c2d6871bcfe80c4","367c69cdd232444e9761f0972ebdef67","d7e3bc642c57435997640e002a9c9f04","4e56753764f24b4a8b5b5c581d0c3b89","6251977459a54cf2a86aa788dda65432","69694ffbc37149a28acceccde288abde"]},"executionInfo":{"elapsed":10294,"status":"ok","timestamp":1729089790961,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"},"user_tz":-480},"id":"sZq2XNAK_--a","outputId":"33811a9a-ebf5-4809-f705-84ed3b1d5466"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:90: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","bf16 tensors are supported.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47a72627d0b64c8588a8f540ecb8b06f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"]}],"source":["# Initialize embeddings\n","embedding_model_name = \"BAAI/bge-small-en-v1.5\"\n","embedding_model_kwargs = {\"device\": \"cuda\"}\n","embedding_encode_kwargs = {\"normalize_embeddings\": True}\n","hf = HuggingFaceBgeEmbeddings(\n","    model_name=embedding_model_name,\n","    model_kwargs=embedding_model_kwargs,\n","    encode_kwargs=embedding_encode_kwargs\n",")\n","\n","# Initialize vector store and retriever\n","vectorstore = Chroma(\n","    persist_directory=#\"/content/drive/MyDrive/UWA/Sem 4/Capstone/Project/vector1\",\n","    #/content/drive/MyDrive/CITS5553_Capstone/vector1\n","    \"/content/drive/MyDrive/UWA/Sem 4/Capstone/Project/vector1\",\n","    embedding_function=hf\n",")\n","retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n","\n","# Check the available device\n","if torch.cuda.is_available():\n","    device = \"cuda\"\n","elif torch.backends.mps.is_available():\n","    device = \"mps\"\n","else:\n","    device = \"cpu\"\n","\n","print(f\"Using device: {device}\")\n","\n","# Check for bf16 support\n","is_bf16_support = False\n","try:\n","    tensor_bf16 = torch.tensor([1.0], dtype=torch.bfloat16, device=device)\n","    is_bf16_support = True\n","    print(\"bf16 tensors are supported.\")\n","except TypeError:\n","    print(\"bf16 tensors are not supported.\")\n","\n","# Quantization configuration\n","# Quantization configuration\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","# Load the base model and tokenizer\n","base_model = \"microsoft/Phi-3.5-mini-instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(base_model)\n","\n","# Load the fine-tuned Phi3 mini model with LoRA\n","model = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=bnb_config,return_dict=True, device_map=device)\n","\n","qlora_model = PeftModel.from_pretrained(model, \"KunalRaghuvanshi/phi3_mini_qlora_chemical_eng\")\n","\n","pipeline = pipeline(\"text-generation\", model=qlora_model, tokenizer=tokenizer, max_new_tokens=1000)\n","llm = HuggingFacePipeline(pipeline=pipeline)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"tnhbv16wAE_X","executionInfo":{"status":"ok","timestamp":1729089795377,"user_tz":-480,"elapsed":385,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[],"source":["# Define the RAG Chat Model class\n","class RAGChatModel:\n","    def __init__(self, retriever, llm, tokenizer, max_token_limit=1200): #count_total_tokens_in_vectorstore(vectorstore, tokenizer)//10):\n","        self.retriever = retriever\n","        self.llm = llm\n","        self.tokenizer = tokenizer\n","        self.max_token_limit = max_token_limit\n","        self.current_token_count = 0\n","        self.template_standard = \"\"\"\n","        <|system|>\n","        Answer the question in detail. Provide all the relevant information based on the provided context.\n","        It is critical that you mention all page numbers where this information is found. Do not skip any page numbers.\n","\n","\n","        Context: {context}\n","\n","        Providing all the page numbers is essential  for the answer.\n","        <|end|>\n","\n","        <|user|>\n","        Question: {question}\n","        <|end|>\n","\n","        <|assistant|>\n","        \"\"\"\n","        self.template_exceeded = \"\"\"\n","        <|system|>\n","        Answer the question in detail; warn that information is not taken from the prescribed textbook and must provide the page numbers where they can find the correct information in the prescribed textbook.\n","\n","        Context: {context}\n","        Providing all the page numbers is essential for the answer.\n","        <|end|>\n","\n","        <|user|>\n","        Question: {question}\n","        <|end|>\n","\n","        <|assistant|>\n","        \"\"\"\n","\n","    def num_tokens_from_string(self, string: str) -> int:\n","        \"\"\"Returns the number of tokens in a text string using the tokenizer.\"\"\"\n","        return len(self.tokenizer.encode(string))\n","\n","    def format_docs(self, docs, full_content=True):\n","        \"\"\"Format the documents to be used as context in the prompt.\"\"\"\n","        if full_content:\n","            return \"\\n\\n\".join(f\"Information in Page number: {(doc.metadata['page']+1)}\\n{doc.page_content}\" for doc in docs)\n","        else:\n","            return \"Information available in prescribed textbook \" + \", \".join(f\"Page number: {doc.metadata['page']}\" for doc in docs)\n","\n","    def get_prompt(self, docs, question):\n","        \"\"\"Generate the prompt based on token count and context formatting.\"\"\"\n","        # Format the context with full content\n","        context = self.format_docs(docs, full_content=True)\n","        total_tokens_in_context = self.num_tokens_from_string(context)\n","\n","        # Add tokens to the running total\n","        self.current_token_count += total_tokens_in_context\n","\n","        # Decide whether to use full content or only page numbers\n","        if self.current_token_count > self.max_token_limit:\n","            print(\"Token limit exceeded. Information from prescribed textbook will not be used.\")\n","            # Reformat context to include only page numbers\n","            context = self.format_docs(docs, full_content=False)\n","            template = self.template_exceeded\n","        else:\n","            template = self.template_standard\n","\n","        # Create the prompt\n","        prompt = template.format(context=context, question=question)\n","        return prompt\n","\n","    def extract_clean_answer(self, raw_output):\n","        \"\"\"Extract only the answer from the raw output.\"\"\"\n","        assistant_tag = \"<|assistant|>\"\n","        if assistant_tag in raw_output:\n","            clean_answer = raw_output.split(assistant_tag)[-1].strip()\n","            return clean_answer\n","        return raw_output.strip()\n","\n","    def ask_question(self, question):\n","        \"\"\"Main function to retrieve relevant docs and generate a response.\"\"\"\n","        # Add fixed request for page numbers to the user's question\n","        question_with_page_request = f\"{question}. Please provide the page numbers in your answer.\"\n","\n","        # Retrieve relevant documents\n","        docs = self.retriever.invoke(question_with_page_request)\n","\n","        # Generate prompt based on token count\n","        prompt = self.get_prompt(docs, question_with_page_request)\n","\n","        # Pass the prompt to the LLM\n","        result = self.llm.generate([prompt])\n","\n","        # Extract the generated text\n","        raw_answer = result.generations[0][0].text\n","\n","        # Get the clean answer\n","        clean_answer = self.extract_clean_answer(raw_answer)\n","\n","        # Display the answer\n","        display(Markdown(clean_answer))\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"MAbb4UbqAVm6","executionInfo":{"status":"ok","timestamp":1729089801211,"user_tz":-480,"elapsed":336,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[],"source":["# Initialize the RAGChatModel\n","rag_chat_model = RAGChatModel(retriever, llm, tokenizer)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2950,"status":"ok","timestamp":1729089806558,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"},"user_tz":-480},"id":"WJOMy2MJyls1","outputId":"d88bc864-5b04-4a1a-f37a-3614d602a8cd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.1.0)\n","Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n","Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.2)\n","Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n","Requirement already satisfied: gradio-client==1.4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.4.0)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n","Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.2)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n","Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n","Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.9)\n","Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n","Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n","Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.32.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.0->gradio) (2024.6.1)\n","Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.0->gradio) (12.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n","Requirement already satisfied: starlette<0.41.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.40.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"]}],"source":["!pip install gradio"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":610},"executionInfo":{"elapsed":3897,"status":"ok","timestamp":1729089813720,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"},"user_tz":-480},"id":"KSMx6Ps3yT5k","outputId":"c19482b0-313d-47d6-d079-3babce23d18a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://51a2e4c96d7076da98.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://51a2e4c96d7076da98.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":11}],"source":["import gradio as gr\n","\n","# Function to shorten the question for the chat history display\n","def get_short_overview(question, answer, max_length=50):\n","    \"\"\"Generate a short summary of the question for the chat history.\"\"\"\n","    return (question[:max_length] + '...') if len(question) > max_length else question\n","\n","# Function for the RAG model interaction\n","def ask_question_gradio(history, question):\n","    \"\"\"Main function to retrieve relevant docs and generate a response.\"\"\"\n","    if not question:  # Check if the question is empty\n","        return history, \"\", \"\"  # Return empty if no question is asked\n","\n","    # Add fixed request for page numbers to the user's question\n","    question_with_page_request = f\"{question}. Please provide the page numbers in your answer.\"\n","\n","    # Retrieve relevant documents using the RAG model\n","    docs = rag_chat_model.retriever.invoke(question_with_page_request)\n","    prompt = rag_chat_model.get_prompt(docs, question_with_page_request)\n","\n","    # Generate the response\n","    result = rag_chat_model.llm.generate([prompt])\n","    raw_answer = result.generations[0][0].text\n","    clean_answer = rag_chat_model.extract_clean_answer(raw_answer)\n","\n","    # Add the question and answer to the conversation history as dicts\n","    history.append({\"role\": \"user\", \"content\": question})  # Add user question\n","    history.append({\"role\": \"assistant\", \"content\": clean_answer})  # Add model answer\n","\n","    # Generate a short summary for the chat history section\n","    short_overview = get_short_overview(question, clean_answer)\n","\n","    # Format the chat history for display in the overview\n","    chat_history = \"\\n\\n\".join([f\"{get_short_overview(q['content'], a['content'])}\" for q, a in zip(history[::2], history[1::2])])\n","\n","    # Return the updated history and chat history for display\n","    return history, chat_history, \"\"  # The empty string clears the input box\n","\n","# Create Gradio Blocks interface\n","with gr.Blocks() as demo:\n","    gr.Markdown(\n","        \"\"\"\n","        <h1 style='text-align: center;'>Q-ChemNerd</h1>\n","        <p style='text-align: center;'>Ask any question and get a response from the RAG model.</p>\n","        \"\"\",\n","    )\n","\n","    with gr.Row():\n","        with gr.Column(scale=1, min_width=200):\n","            gr.Markdown(\"### Chat History Overview\")\n","            history_display = gr.Textbox(label=\"Chat History\", lines=20, interactive=False)  # Non-editable\n","\n","        with gr.Column(scale=2):\n","            chatbot = gr.Chatbot(label=\"QLora ChemNerd Chat\", type='messages')  # Ensure type is 'messages'\n","            user_input = gr.Textbox(placeholder=\"Ask your question...\", label=\"Type your message here:\")\n","            submit_button = gr.Button(\"Send\")\n","\n","            history_state = gr.State([])\n","\n","            submit_button.click(\n","                ask_question_gradio,\n","                inputs=[history_state, user_input],\n","                outputs=[chatbot, history_display, user_input],  # Update chatbot and chat history\n","                scroll_to_output=True\n","            )\n","\n","            user_input.submit(\n","                ask_question_gradio,\n","                inputs=[history_state, user_input],\n","                outputs=[chatbot, history_display, user_input],  # Update chatbot and chat history\n","                scroll_to_output=True\n","            )\n","\n","# Launch the Gradio interface\n","demo.launch(share=True)\n"]},{"cell_type":"markdown","metadata":{"id":"c-1jL_wcZbB7"},"source":["How many stages are required to distil toluene from decane at atmospheric conditions?\n","\n","To heat a dodecane fluid with steam, what type of heat exchanger should be applied and why?\n","\n","What methods are available to capture CO2 from air at dilute concentrations?"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"5_rtxszlAY2E","outputId":"8c9b5206-102c-46ca-c307-87ff34a54a13","executionInfo":{"status":"error","timestamp":1729090553487,"user_tz":-480,"elapsed":83461,"user":{"displayName":"Kunal R R","userId":"08771470267005146834"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Welcome to the RAG Chat Model! Ask any question (type 'exit' to quit):\n","\n","\n","\n","Your question:  exit\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The instruction requires a detailed response that includes all relevant information from the provided context, specifically mentioning the page numbers where this information can be found. Since the actual content and page numbers are not provided in your query, I will create a hypothetical answer that would fit the instruction if the content were available.\n\nIn the context of understanding the importance of citing page numbers in academic and research settings, it is crucial to recognize that page numbers serve as a navigational tool, allowing readers to locate the original source of information quickly. This practice not only enhances the credibility of the research by demonstrating thoroughness and attention to detail but also facilitates the verification process for interested readers.\n\nFor instance, when referencing a study on the impact of climate change on marine biodiversity, including the page number where the specific data on coral reef degradation was found (e.g., \"Smith et al., 2021, p. 45\") provides a clear path for readers to access the original source. This practice is particularly important in fields where data and findings are rapidly evolving, as it allows for the verification of claims and the exploration of the research's depth.\n\nMoreover, the inclusion of page numbers in citations adheres to the academic standards set by various style guides, such as APA, MLA, and Chicago, which mandate the provision of detailed bibliographic information to ensure the traceability of sources. Failure to include page numbers can lead to difficulties in locating the original source, potentially undermining the research's integrity and the author's credibility.\n\nIn conclusion, the practice of including page numbers in academic citations is a fundamental aspect of scholarly communication. It not only aids in the efficient navigation of research materials but also upholds the principles of academic integrity and transparency. For a comprehensive understanding of this practice, readers are encouraged to consult the relevant sections of their academic style guides, which typically include detailed instructions on how to format citations, including the inclusion of page numbers.\n\nHypothetical Page Numbers:\n\n- Introduction to Academic Citations: p. 1\n- Importance of Page Numbers in Citations: p. 2\n- Navigating Academic Research with Page Numbers: p. 3\n- Adhering to Academic Style Guides: p. 4\n- Case Study: Including Page Numbers in Citations: p. 5\n- Conclusion: p. 6\n\nPlease note, these page numbers are illustrative and not from an actual text. In a real scenario, the answer would include the specific page numbers from the text where the relevant information is found."},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n","\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-22336b935923>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your question: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exiting the chat.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["# Start the interactive chat\n","print(\"Welcome to the RAG Chat Model! Ask any question (type 'exit' to quit):\")\n","while True:\n","  print(\"\\n\\n\")\n","  question = input(\"Your question: \")\n","  if question.lower() == 'exit':\n","    print(\"Exiting the chat.\")\n","    break\n","  rag_chat_model.ask_question(question)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1A13S1qWGp-bcdFvYRer_Q65wxl-qOuoD","timestamp":1729089355931},{"file_id":"1ctZe_beYWunSL3f3qnES85zs_X997Jvz","timestamp":1728709811945},{"file_id":"1kU74NQOgketqIUjiLRlhmsSdRsGKDOCF","timestamp":1728709092928}],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"47a72627d0b64c8588a8f540ecb8b06f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_494cde9578904a14af109517dd1f06ec","IPY_MODEL_fd69bda058c541d0b17800797b7301ab","IPY_MODEL_78a541c71ad34026bbf4727ac7ea7999"],"layout":"IPY_MODEL_006c1689411e4f4fa2d03656e5f86f48"}},"494cde9578904a14af109517dd1f06ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3154a1a68f9243549c2d6871bcfe80c4","placeholder":"​","style":"IPY_MODEL_367c69cdd232444e9761f0972ebdef67","value":"Loading checkpoint shards: 100%"}},"fd69bda058c541d0b17800797b7301ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7e3bc642c57435997640e002a9c9f04","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e56753764f24b4a8b5b5c581d0c3b89","value":2}},"78a541c71ad34026bbf4727ac7ea7999":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6251977459a54cf2a86aa788dda65432","placeholder":"​","style":"IPY_MODEL_69694ffbc37149a28acceccde288abde","value":" 2/2 [00:05&lt;00:00,  2.79s/it]"}},"006c1689411e4f4fa2d03656e5f86f48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3154a1a68f9243549c2d6871bcfe80c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"367c69cdd232444e9761f0972ebdef67":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7e3bc642c57435997640e002a9c9f04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e56753764f24b4a8b5b5c581d0c3b89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6251977459a54cf2a86aa788dda65432":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69694ffbc37149a28acceccde288abde":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}